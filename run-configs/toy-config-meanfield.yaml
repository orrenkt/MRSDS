
data:
  data_source: "mydata"
  tlim: null  # reject trials over tlim timepoints
  inputs: 2   # us input length. 2 is default, will zero pad if no inputs are provided

model:
  uz: False  # pass inputs (us) to discrete transition
  sz: False  # pass trial stages to discrete transition (if stages provided)
  xz: True   # pass continuous latents (xs) to discrete transition p(z|z,x)
  ux: True   # pass inputs (ux) to continuous transition p(x|x,u) 
  xic:
    triangular_cov: False
    trainable_cov: True
    layers: [4] # final layer has var size equal to latent x dim
    activations: ["relu", null]
  xtransition:
    # For switching dynamics. If zcond True, uses single x dynamics net with
    # z as conditioning input. If False, uses k (number discrete states) dynamics networks
    # Using zcond means fewer model parameters and sometimes works better for k>1.
    zcond: False
    triangular_cov: False
    trainable_cov: True
    dynamics:
      # Communication can be additive, additive-linear, or conditional
      # additive (default): x^k_{t+1} = f_{kk}(x^k_t) + f_{kj}(x^j_t) + noise
      # additive-linear: same, but f_{kj} is linear
      # conditional: x^k_{t+1} = f_{kk}(x^k_t) + ff_{kj}(x^k_t, x^j_t) + noise
      # in conditional the communication is conditioned on local activity x^k
      # note this is underconstrained / harder to interpret since both are now function of x^k
      communication_type: "additive" # alt: additive-linear, conditional
      # Same as for dynamics. Additive means f_{ku}(u_t), conditional means f_{ku}(x^k_t, u_t)
      input_type: "additive" # alt: conditional
      # seperate inputs means we learn a seperate additive function f_{ku} for each input.
      # this is important if you want to get seperate messages from each input,
      # otherwise they are combined.
      input_func_type: 'seperate' # alt: together
      # Final dynamics layer gets variable size equal to latent dim x^k, no need to specify
      # here. we do specify the nonlinearity, null (ie linear layer) by default
      # to allow negative x values.
      layers: [32, 32]
      activations: ["relu", "relu", null]
    inputs:
      # Same as for dynamics, no need to specify final layer, set equal to latent dim x^k.
      # For toy config can use linear input mapping, otherwise add layers here.
      layers: [] # final layer has var size
      activations: [null]
  emissions:
    triangular_cov: False
    trainable_cov: True
    dist: "gaussian" # Alt: poisson
    # For toy config can use linear emissions, otherwise add layers here. 
    layers: []
    activations: [] #'relu']
    final_activation: null # poisson alt: exp, softplus

inference:
  triangular_cov: False
  trainable_cov: True
  transformer: True
  embedding: # To embed each region's observations before input to transformer encoder 
    layers: [8]
    activations: ["relu"]
  transformer_params:
    head_size: 35
    num_heads: 3
    ff_dim: 4
    num_transformer_blocks: 1
    mlp_units: [32]
  num_samples: 1  # Number of posterior samples to use in inference call
   
training:
  batch_size: 64
  # Set to True only for mean field (mrsds) inference.
  # Continues training dynamics only with frozen inference net (better dynamics, generation)
  # The params num_steps_do, lr_warmup_steps_do specify training for this second stage.
  # As a rule of thumb, you can set number of do steps between 0.75-1.5 times num_steps.
  dynamics_only: True 
  # Number of training iterations
  num_steps: 2001
  num_steps_do: 3001
  learning_rate: 1.e-2
  # Warmup steps for learning rate, best to set as fraction of num_steps
  # As a rule of thumb set this between 1/4 for low overall steps and 1/10 for high overall steps
  # eg 500 for 2000 steps, 2000 for 15000 steps
  lr_warmup_steps: 500
  lr_warmup_steps_do: 500
  # For mean field inference only. Temperature is applied to discrete transition
  # z_t ~ Softmax( p(z[t] | z[t-1], x[t-1]) / temp)
  # This gets annealed over the course of training. Increasing temperature 
  # increases the posterior entropy over discrete states, want to encourage use
  # of all states early in training.
  temperature_annealing: True
  t_init: 0
  t_min: 1.0
  annealing_rate: 0.99
  annealing_steps: 500
  annealing_kickin_steps: 0
  # For structured inference only (svae, s-svae), this param is ignored by mean field.
  # beta max sets highest value of beta, which upweights likelihood vs reconstruction
  # term in elbo. this roughly serves a similar purpose as second stage of dynamics training
  # for mean field inference. beta is annealed to 1 over training. the right beta level
  # will depend on your data, use a larger beta for datasets with more neurons.
  # eg 100 for small toy dataset, 1000-5000 for bigger dataset.
  # note that setting beta high can result in worse cosmoothing results, but better generation
  beta_max: 0
  beta_warmup_steps: 0
  # Can set these higher (eg 1k+) for longer training models, and log more often than save
  log_steps: 500 # How often to evaluate perf, print to log. Also tracks over training. 
  save_steps: 500 # How often to checkpoint model, eval latents etc and save to latents.mat
  test_perc: 0.1 # Train/test split.
  # Whether to run dropout training, which approximates cosmoothing.
  # This improves cosmoothing performance and generally better for real datasets
  # may not matter much for small toy datasets
  dropout: False  
  drop_perc: 0.25
  dropout_trial_perc: 0.5
